\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Predicting the Success of a Song\\
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Ronald Ankner}
\IEEEauthorblockA{\textit{Department of Electrical Engineering} \\
\textit{Stevens Institute of Technology}\\
Hoboken, NJ \\
rankner@stevens.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Amit Singh}
\IEEEauthorblockA{\textit{Department of Electrical Engineering} \\
\textit{Stevens Institute of Technology}\\
Hoboken, NJ \\
asingh61@stevens.edu}
}

\maketitle

\begin{abstract}
The goal of this study is to understand the feasibility of predicting the success of a song, but more so, once a stable prediction model is created, determine what factor play the biggest role in a song's success and why. To accomplish, 1.8 Million songs and their corresponding features were compiled into a data set constituting over 6\% of all songs available on Spotify and a large percentage of all songs produced in America from 1900-2019. Several machine learning techniques were applied to problem, trained on a wide range of features, with varying success.
\end{abstract}

\begin{IEEEkeywords}
Artificial Neural Network, Decision Tree, Feature Analysis, Audio Analysis, Unsupervised Learning
\end{IEEEkeywords}

\section{Introduction}
Songs play at the very heart strings of every human. Songs that resonate have the power to capture some of the strongest human emotions. Strong lyrics have the power to uniquely define a shared human perspective. Despite this, whether a song becomes a hit or not seems largely random; there's no identifiable rhyme or reason. The goal of this study is to capture that rhyme and reason. Is there a pattern? Can machine learning and other pattern recognition techniques pull out the under-lying features that drive not only the success of a song, but why it resonates with the majority of human beings?

To accomplish, a custom data set of over 1.8 Million songs was compiled. Features ranging from Spotify-derived features to lyrics to raw audio files are employed to find the driving patterns.

\section{Tools Used}
\begin{itemize}
\item Sklearn
\item Matplotlib
\item Numpy
\item Pandas
\item Spotipy
\item Statsmodels
\item Urllib
\end{itemize}

\section{Related Work}
\subsection{Spotify Feature Analysis}
SECTION TO COME
\subsection{Audio Analysis}
SECTION TO COME
\subsection{Lyric Analysis}
SECTION TO COME

\section{Data Collection}
Data was assembled uniquely for our project as there were no existing data sets that met our needs. Several scripts were written to compile data from various sources to make a final data set of over 1.8 Million songs, a data set which constitutes about 6\% of all songs available on Spotify and a large majority of songs written in the United States from 1900-present.

Data was compiled from:
\begin{itemize}
\item Wikipedia
\item Other Websites
\item Spotify
\end{itemize}

The full data set took over 24 hours of run time alone to compile, aside from the several days of script development.

\subsection{Scraping the Internet}
The first step was creating a custom script that scraped a Wikipedia list of song names by year, starting from 1946 and ending at 2019. The web page that was scraped can be found here: \url{https://en.wikipedia.org/wiki/Category:Songs_by_year}

The next step was scraping a separate web page for names of American artists during that time period. The link for the artists is: \url{https://www.udiscovermusic.com/artists-a-z/}

From there, the “Successful” songs needed to be found. To accomplish this, again, a separate custom script was made to scrape the billboard songs off the Wikipedia page: \url{https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_XXXX} where ‘XXXX’ was dynamically changed to reflect the year that was being scraped.

\subsection{Cross Referencing with Spotify}
Armed with lists of songs, artists, and billboard hits, the effort was then focused on interfacing with Spotify. In order to do this, an open source code-base called “spotipy” was employed to interface easier with the Spotify web API. The song search effort on Spotify was broken up into 3 segments and required unique scripts for each:
\begin{itemize}
\item Artist - with the artist name, a query was done on Spotify to find the artist ID. With that ID, all albums for the artist were found, and information about all songs on each album was collected and stored.
\item Billboard Hits - because billboard status is the main target variable for this project, when cross referencing the Wikipedia list with Spotify, attention was paid to the similarity of the information. So, the song name from the internet was used as a query for spotify. 50 results were returned from that search. For each of the 50 songs, the year, song name, and artists of the song (which were obtained by scraping the hyperlink for the song title from Wikipedia) were checked against the Spotify song in question. A similarity measure was calculated for each of the 50 results, and the first, most similar result was chosen as the song on Spotify. This was done for all 6000+ billboard hits collected from Wikipedia
\item All Songs - because the “not hits” portion of the data set is not as selective as the hits, the song names obtained from Wikipedia were used as a query on Spotify, and all 50 results were recorded to the database. This was done for all 84000+ song titles obtained from Wikipedia.
\end{itemize}
Several pieces of information were collected from Spotify for each query approach:
\begin{itemize}
\item Song Name
\item Artist(s)
\item Spotify ID  - Unique Identifier which can be used to get a wealth of other information)
\item Release Date
\item Popularity - Spotify Metric that mostly reflects total number of listens and how frequent those listens are
\item Duration
\end{itemize}
The three lists are then combined and duplicates are removed, creating the list of roughly 1.8 million songs.

\subsection{Extracting Song Features from Spotify}
The final step in the collection process is collecting the audio features from Spotify for each of the songs. Because 50 ID’s can be called at a time, the audio features for the 1.8 million songs are collected in chunks of 50. The audio features given by spotify are: 'acousticness', 'danceability', 'energy',  'instrumentalness', 'key',  'liveness',  'loudness', 'mode', 'speechiness', 'tempo', 'time signature', and 'valence'.

The final compiled and completed data set is shown in a pandas DataFrame Summary in Fig.~\ref{fig:dataframe}.
\begin{figure}[htbp]
\centerline{\includegraphics[width=6cm, height=7cm]{images/Song_Dataset_Dataframe.png}}
\caption{Pandas Dataframe Description of Full Data Set}
\label{fig:dataframe}
\end{figure}

\section{Feature Analysis}
\subsection{Histogram Analysis}
To analyze the strength of features, histograms were plotted for each feature by group: successes and not-successes.

As shown in Fig.~\ref{fig:hist_year} and Fig.~\ref{fig:hist_pop}, Year and Popularity seem to be very strong features, as the histogram distributions between the successes and the not successes have different shapes. Key (Fig.~\ref{fig:hist_key}) and Danceability (Fig.~\ref{fig:hist_dance}), however, seem to be weak features as the histograms between the hits and not hits are almost identical, and might only serve to confuse the model if left in the feature set. 

This analysis was conducted for all features in the data set.

\begin{figure}[htbp]
\centerline{\includegraphics[width=6cm]{images/feat_year.png}}
\caption{Year Histogram}
\label{fig:hist_year}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=6cm]{images/feat_pop.png}}
\caption{Popularity Histogram}
\label{fig:hist_pop}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=6cm]{images/feat_key.png}}
\caption{Key Histogram}
\label{fig:hist_key}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=6cm]{images/feat_dance.png}}
\caption{Danceability Histogram}
\label{fig:hist_dance}
\end{figure}

\subsection{Multivariate Regression P-value Analysis}
Doing a multivariate regression on the features and then looking at the p-values gives a good understanding of the significance of each feature. Low p-values, close to 0, show that a feature is statistically significant. The p-value is calculated based on the coefficient’s statistical significance with respect to the null hypothesis coef = 0, or in layman's terms, the feature does not contribute to the model. As shown in Fig.~\ref{fig:regress} Danceability, Key, and Time signature, all have above 0 p-values and therefore can be considered insignificant to the model. Removing these variables will reduce noise and confusion within models, and furthermore, this result corroborates the visual inspection of the histograms. 

\begin{figure}[htbp]
\centerline{\includegraphics[width=8.5cm]{images/regress_before.png}}
\caption{Feature Multivariate Regression}
\label{fig:regress}
\end{figure}

\section{Machine Learning Attempts}
The initial steps of machine learning work has focused on predicting the Success of a song based on the binary classification is it a billboard hit or not: billboard hits are considered a success, while not billboard songs are not successes. However, using this as the target variable has complicated the training due to a severely imbalanced data set. There are only roughly 5800 billboard hits in the data set of over 1.8 million, resulting in a success to not-success ratio of 310:1. 0.3\% of the data set is a success. This presents issues as the best way for a neural network to improve overall losses is to essentially assume the song is not a hit every time, yielding above 99.7\% accuracy on the set as a whole and 100\% success on detecting not hits, but 0\% on detecting hits.

To remedy this issue the not hits are randomly reduced to 25\% of their full size, yielding a full data set size of 472,000 and driving the success rate of the data set to 1.23\%, making training more manageable. Two well trained models for ANN’s and Decision Trees on the reduced data set are shown in Fig.~\ref{fig:ML_ANN} and Fig.~\ref{fig:ML_TREE} respectively.

The results displayed in Fig.~\ref{fig:ML_ANN} and Fig.~\ref{fig:ML_TREE} show that the ANN does a surprisingly good job of correctly classifying hits in both the test set and the training set. The Decision Tree, however, has a rough time achieving high hit classification. Increasing the complexity of the tree only serves to increase the in-sample classification, while not helping the out-sample. The tree does keep consistently high accuracy in classifying not hits. 

To compare the trained models further, a script was written to generate a data set of 10000 random songs from the set of 1.8 million, irrespective of their success classification. Running the script several times shows that no matter the random data set, the accuracies of both models stay relatively constant, emphasizing that the 75\% reduced data set is still decently representative of the full 1.8 million song data set. More interesting conclusions appear when looking at the specific results of one of these random data sets as shown in Fig.~\ref{fig:ML_COMPARE}.

From the data set of 10000 randomly drawn, only 25 hits are present. Despite this, the ANN does a good job of correctly classifying them: 22 out of the 25. The decision tree,contrastingly, correctly finds only 8 of the 25. However, when looking at the Misclassified Hits, the ANN has significantly more misclassified hits: 1241 for the ANN and only 30 for the decision tree. Calculating the percentage of classified hits that are actually hits yields the “Hit Success Rate”, which for the ANN is an abysmal 1.74\%, but for the decision tree is 21.05\%. For comparison, randomly choosing hits would yield, at most, a Hit Success Rate of 0.3\%. So the ANN, only marginally raises the chances of correctly classifying a hit, while the decision tree significantly increases those chances.

Both have a use though. Let's take the use case of a production studio choosing song previews to sign for an album. If the production studio does not see many new artists to sign, they would want the ANN, as it has a high chance of finding all the possible hits they screen, and because they do not screen a ton of songs, they will not sink too much money in non-hits. If the production company, by contrast, screens a ton of songs, they would want the decision tree as it will minimize their losses and if they miss a few hits it will not make a difference because they will be screening more possible hits.

There are plans to combine the two models using a Bayes ensemble method and the confusion matrices to get the best of both models. 


\begin{figure}[htbp]
\centerline{\includegraphics[width=5cm]{images/ANN_results.png}}
\caption{Artificial Neural Network Results}
\label{fig:ML_ANN}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=5cm]{images/TREE_results.png}}
\caption{Decision Tree Results}
\label{fig:ML_TREE}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=8.5cm]{images/binary_classifier_comparison.png}}
\caption{ANN and Decision Tree Comparison}
\label{fig:ML_COMPARE}
\end{figure}


\section{Results (To Come When Done)}
TO BE DONE BEFORE FINAL PROJECT SUBMISSION

\section{Future Work}
\subsection{Popularity as a Target Variable}
TO BE DONE BEFORE FINAL PROJECT SUBMISSION Fig.~\ref{fig:Target_POP}
\begin{figure}[htbp]
\centerline{\includegraphics[width=7cm]{images/popularity_as_target.png}}
\caption{popularity Class Breakdown}
\label{fig:Target_POP}
\end{figure}
\subsection{Online Learning: Lyrics and Spotify Analysis API}
TO BE DONE BEFORE FINAL PROJECT SUBMISSION
\subsection{Unsupervised Learning}
TO BE DONE BEFORE FINAL PROJECT SUBMISSION
\subsection{Successful Model Analysis and Visualization}
TO BE DONE BEFORE FINAL PROJECT SUBMISSION

\section{Conclusions}
TO BE DONE BEFORE FINAL PROJECT SUBMISSION

\section*{References}

TO BE DONE BEFORE FINAL PROJECT SUBMISSION \cite{b1}.

\begin{thebibliography}{00}
\bibitem{b1} TO BE DONE BEFORE FINAL PROJECT SUBMISSION
\end{thebibliography}

\end{document}
